# AMD Diffusion Distillation
This repository provides training recipes for our Nitro models, that are distilled from public diffusion models.

For SDv2.1-base model, we achieve FLOP reduction of 95.95%, but only at the cost of 2.5% decreased CLIP score. The FID of the distilled model is even better than the full model, decreased by 14.23%.

| Model    | FID &darr; | CLIP &uarr; |FLOPs| Latency on MI250 (sec)
| :---: | :---: | :---: | :---: | :---:
| SDv2.1-base 50 steps (cfg=7.5) | 25.47   | 0.3286 |83.04 | 4.94
| SDv2.1-base-Nitro 1 step | 26.04     | 0.3204|3.36 | 0.18

For PixArt-Sigma model, our one-step PixArt-Sigma-Nitro achieves a 90.93% reduction in FLOPs at the cost of 3.7% lower CLIP score and 10.56% higher FID.

| Model    | FID &darr; | CLIP &uarr; |FLOPs| Latency on MI250 (sec)
| :---: | :---: | :---: | :---: | :---:
| PixArt-Sigma 20 step | 34.14   | 0.3289 |187.96 | 7.46
| PixArt-Sigma-Nitro 1 step | 37.75     | 0.3167|17.04 | 0.53

## Environment

### Docker image
Pull the following docker from [docker hub](https://hub.docker.com/r/rocm/pytorch)

``` 
docker pull rocm/pytorch:rocm6.1.3_ubuntu22.04_py3.10_pytorch_release-2.1.2 
```

### Dependencies
install the core python libraries by

```
pip install diffusers==0.29.2 transformers accelerate wandb torchmetrics pycocotools torchmetrics[image] open-clip-torch
```

## Synthetic data generation

We distill our models simply using synthetic data generated from the original models. In order to do so, we need a prompt dataset as the input. In this project, we used prompts from [DiffusionDB](https://huggingface.co/datasets/poloclub/diffusiondb). Please extract prompts from the dataset and put it in a txt file where each line corresponds to a prompt. 

We provide a sample list ```data/sample_prompts.txt``` for a demo.

#### Generating data for SD v2.1-base
```
bash scripts/run_gen_data.sh
```

#### Generating data for Pixart_alpha
```
bash scripts/run_gen_data_pixart.sh
```

Please remember to correctly set **"PROMPT_PATH"** and **"OUT_FOLDER"** in the scripts.


## Train models
To distill a model, simply run scripts:
```
bash scripts/run_train.sh # for sdv2.1_base
```


It is important to correctly set **"DATA_ROOT"** that was generated by the previous step. Also please correctly set the training parameters of accelerate to use correct number of gpus and batchsize. Please also refer to [Accelerate CLI](https://huggingface.co/docs/accelerate/en/package_reference/cli) for more details.

## Generate images
For SDv2.1-base, run:
```
from diffusers import (DDPMScheduler,
                       DiffusionPipeline)
import torch


scheduler = DDPMScheduler.from_pretrained("stabilityai/stable-diffusion-2-1-base", subfolder="scheduler")
pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1-base",
                                        scheduler=scheduler)
ckpt_path = 'CKPT_FOLDER/pytorch_model.bin'
unet_state_dict = torch.load(ckpt_path)
pipe.unet.load_state_dict(unet_state_dict)
pipe = pipe.to("cuda")
image = pipe(prompt='a photo of an astronaut riding a horse on mars',
             num_inference_steps=1,
             guidance_scale=0,
             timesteps=[999]).images[0]

```

## Evaluation

**COCO dataset**
Download COCO val2017 images from [Here](http://images.cocodataset.org/zips/val2017.zip) and annotation from [Here](http://images.cocodataset.org/annotations/annotations_trainval2017.zip)

Create a root folder called "coco" and unzip these two files into this folder. So the folder looks like:

```
coco/
├── val2017/
└── annotations/

```
To evaluate the model, run:
```
bash scripts/run_eval.sh
```

Please correctly set variables in this script including "COCO_ROOT", "CKPT_PATH", "MODEL", etc. The script will generate 5k images based on 5k unique given prompts from COCO val2017 dataset, and the FID and CLIP scores are calulated based on the generated images.
## License

Copyright (c) 2024 Advanced Micro Devices, Inc. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.