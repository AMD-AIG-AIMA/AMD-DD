# AMD-DD
This repository provides training recipes for distillating diffusion models to few-step or one-step versions. 

For SDv2.1-base model, we achieve FLOP reduction of 95.95%, but only at the cost of 2.5% decreased CLIP score. The FID of the distilled model is even better than the full model, decreased by 14.23%.

| Model    | FID &darr; | CLIP &uarr; |FLOPs| Latency on MI250 (sec)
| :---: | :---: | :---: | :---: | :---:
| SDv2.1-base 50 steps (cfg=7.5) | 25.47   | 0.3286 |83.04 | 4.94
| Our distilled model 1 step | 26.04     | 0.3204|3.36 | 0.18

## Environment

### Docker image
Pull the following docker from [docker hub](https://hub.docker.com/r/rocm/pytorch)

``` 
docker pull rocm/pytorch:rocm6.1.3_ubuntu22.04_py3.10_pytorch_release-2.1.2 
```

### Dependencies
install the core python libraries by

```
pip install diffusers==0.29.2 transformers accelerate wandb torchmetrics pycocotools torchmetrics[image] open-clip-torch
```

## Synthetic data generation

We distill our models simply using synthetic data generated from the original models. In order to do so, we need a prompt dataset as the input. In this project, we used prompts from [DiffusionDB](https://huggingface.co/datasets/poloclub/diffusiondb). Please extract prompts from the dataset and put it in a txt file where each line corresponds to a prompt. 

We provide a sample list ```data/sample_prompts.txt``` for a demo.

#### Generating data for SD v2.1-base
```
bash scripts/run_gen_data.sh
```

#### Generating data for Pixart_alpha
```
bash scripts/run_gen_data_pixart.sh
```

Please remember to correctly set **"PROMPT_PATH"** and **"OUT_FOLDER"** in the scripts.


## Train models
To distill a model, simply run scripts:
```
bash scripts/run_train.sh # for sdv2.1_base
```


It is important to correctly set **"DATA_ROOT"** that was generated by the previous step. Also please correctly set the training parameters of accelerate to use correct number of gpus and batchsize. Please also refer to [Accelerate CLI](https://huggingface.co/docs/accelerate/en/package_reference/cli) for more details.

## Generate images
For SDv2.1-base, run:
```
from diffusers import (DDPMScheduler,
                       DiffusionPipeline)
import torch


scheduler = DDPMScheduler.from_pretrained("stabilityai/stable-diffusion-2-1-base", subfolder="scheduler")
pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1-base",
                                        scheduler=scheduler)
ckpt_path = 'CKPT_FOLDER/pytorch_model.bin'
unet_state_dict = torch.load(ckpt_path)
pipe.unet.load_state_dict(unet_state_dict)
pipe = pipe.to("cuda")
image = pipe(prompt='a photo of an astronaut riding a horse on mars',
             num_inference_steps=1,
             guidance_scale=0,
             timesteps=[999]).images[0]

```

## Evaluation
TODO

## License

Copyright (c) 2024 Advanced Micro Devices, Inc. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.